
Gigascale Points-to Analysis Artifact Evaluation
================================================

This guide explains how to setup the evaluation framework for the Gigascale points-to analysis. The guide is divided into two sections:

 * **Getting Started.** Describes the exact procedure to acquire, test, and execute the framework.
 * **Step-by-step.** Describes the process used in our evaluation, and how future researchers may replicate our results.

For an online version of this guide, please visit <http://rp-www.cs.usyd.edu.au/~nhol8058/projects/Gigascale/Artifact_Evaluation.html>

Getting Started
---------------

Note that artifact reviewers will be provided with a virtual machine with initial steps already completed.  If you have a virtual machine image, please skip to step *Test the framework*

 0. *Install dependencies.* The framework is designed to run on Ubuntu-like environments, and is tested on `Lubuntu-15.04-desktop`. The framework is written in Java and must have `javac` `java` and `ant`, as well as basic utilities `tar`, `sed`, `python`, etc.

        > sudo apt-get install default-jdk
        > sudo apt-get install ant

    The datalog engine [Logicblox](http://www.logicblox.com/) (version 3.9) was compared in our evaluation.  This software is commercially available, or an academic license can be sought.

 1. *Acquire the framework.* Download the components of the framework to your testing computer:

    [gigascale-1.4.tar.bz](https://drive.google.com/uc?id=0B_mPWKbbkfRnYl9zUWtCQTV0d28&export=download) [md5sum: 71ae6559aae8de2f74c9c74a3d75ca22]

    Then extract it using `tar`

        > tar -xjvf gigascale-1.x.tar.bz

 2. *Test the framework.* Begin with a simple run which prints statistical information about one of the benchmarks:
    
        > cd gigascale-1.x
        > ./Table1-2.sh -i datasets/dacapo9/sunflow/

    This will compile the framework, and execute a statistical analysis script `nz.ac.massey.gp2.pointsto.benchmarks.MeasureAll`. The script logs output data (printed to screen), and the `Table1-2.sh` script captures the output and formats it in a csv file, and prints it to the screen.

    Next we must test that the memory and runtime performance experiments are working. Define the location of the Logicblox installation (if you have it):

        > cd <logicblox directory>
        > export LOGICBLOX_HOME=`pwd`

    Where `$LOGICBLOX_HOME/bin/bloxbatch -v` is a valid command. Furthermore, Logicblox requires significant amounts of shared memory to be available in `/dev/shm`, you can either apportion more shared memory, by modifying `/etc/fstab` to include (or replace) a line:

        tempfs /dev/shm tempfs defaults,size=<amount> 0 0
    
    Then restart your machine. Otherwise quell the warning messages:

        > export LB_PAGER_FORCE_START=1
        > export LB_MEM_NOWARN=1

    If you do not have Logicblox, please exclude logicblox evaluation from all future executions with the `-l` flag. Now run the framework:

        > cd gigascale-1.x
        > ./Table3.sh -i datasets/dacapo9/sunflow/

    This may take several minutes. If it succeeds, a table will be displayed containing runtime and memory-usage statistics for WL, DP, LB and TC points-to analyses of the sunflow benchmark, similar to Table 3:

        benchmark WL-time WL-mem DP-time DP-mem LB-time ...
        sunflow   0.93    121    1.04    143    2.10    ...

 4. *Run the complete framework.* Execute all experiments on the DaCapo 2009 benchmarks by executing the `./run` script (takes around 20 minutes):

        > cd gigascale-1.x
        > ./run -i datasets/dacapo9/

    This will execute the experimental evaluation on all benchmarks except OpenJDK.  We exclude OpenJDK from complete analysis as it takes too long, though we encourage you to verify the 'less than a minute' claim with:

        > ./run -wdli datasets/openjdk/

    Which performs statistical analysis, bridge comparison, refinement comparison, and solving via TC on the OpenJDK benchmark.

Step-by-step
------------

### Experimental Evaluation ###

Each table presented in our paper can be individually replicated by the `TableXXX.sh` scripts provided. When attempting to replicate our results please consider the following:

 * Execution occurs on a non-networked, bare-metal, default-installation. Our performance numbers should not be compared with virtual machines or multi-user systems/servers.
 * Execution times and memory consumptions are reported as an average of the 5 best amongst 10 execution runs.  The `Table3.sh` script only performs one exection of each benchmark/implementation.
 * The refinement steps needed by bottom-up and top-down refinement (`Table5.sh`) are subject to nondeterministic ordering of the potential bridges. We report the minimum number of steps in both cases.
 * We determine that WL and DP implementations time-out on the OpenJDK benchmark after 2 hours. The scripts do not automatically time-out, so the evaluator must manually `kill -9` the process when they are satisfied that it takes too long.
 * Logicblox execution on the OpenJDK dataset can take **hours** to complete. On our Intel(R) Xeon(R) E5-24500 32-core CPU with 128GB RAM the execution times averaged 42 minutes.

### Artifact Structure ###

The artifact is distributed with all necessary sources and input datasets to reproduce the results reported in our paper included. An overview of the key components follows:

 * **datasets** contains input datasets for the `dacapo9/*/` and `openjdk/`. Testdata and historic benchmarks for DaCapo 2006 can be found in `.testdata` and `.dacapo6` respectively.
 * **java** contains the java sources and dependencies for the worklist (wl), difference-propagation (DP), and gigascale (TC) implementations.
 * **lb** contains Logicblox execution scripts and logic files:
   * **lb.sh** is the main logicblox script, this script sets up a single Logicblox execution.  Examine it for details on this process.
   * **vpt.logic** is the primary logic file for points-to analysis in Logicblox.
 * **memusg** is the monitor used to periodically check the memory footprint of experiment executions.
 * **run.sh** is the main script for generating experimental data as reported in our work.
 * **TableXXX.sh** scripts call `run.sh` with specific flags to generate one or two tables exactly as seen in our work.

### Input Data ###

Input data for our framework is formatted as csv files without headers, and distributed in the `datasets` subdirectory.  A benchmark problem is a single folder with Alloc.csv, Assign.csv, Load.csv and Store.csv.  The problem may also include Cast.csv, which we treat as additional Assign data, and/or VarPointsTo.csv, which is an independently generated correct solution. For the DaCapo benchmarks, cast data has been added to assign already. The format, "filename `code` *csv-format*", is:

 * Alloc.csv `var = new obj();` *var,obj*
 * Assign.csv `dst = src;` *src,dst*
 * Cast.csv `dst = (type)src;` *src,dst,type*
 * Load.csv `dst = base.field;` *base,dst,field*
 * Store.csv `base.field = src;` *src,base,field*
 * VarPointsTo.csv `obj -> var` *obj,var*

Additionally, simple test-cases are provided in `datasets/.testdata/*/` which include java code, a picture of the points-to graph, and the relevant files to assist in understanding the input format.

**How do I make the input data?** Generating the input data is out-of-scope for our research, we assume the graph problem is given and we simply solve it. The problems in our evaluation were generated using the open-source tools [doop](http://doop.program-analysis.org/), [tamiflex](http://secure-software-engineering.github.io/tamiflex/), and [soot](https://github.com/Sable/soot). The OpenJDK dataset was generated using a proprietary tool by Oracle inc.

### Java Sources ###

The artifact contains source versions of all the analysis implementations compared in our work. The java subfolder contains all the libraries necessary to execute the WL, DP and TC implementations. Example scripts on how to interface and run our framework can be found in `benchmarks/nz/ac/massey/gp2/pointsto/benchmarks/Measure*.java`. In general, any analysis which fulfils the contract of `src/nz/ac/massey/gp2/pointsto/AbstractPointsToReachability.java` can be used as a points-to-analysis implementation.

You can compile the java sources manually with:

    > cd java
    > ant compile

You can execute an experiment manually (in this example, measure the single-source query-time using bottom-up and top-down strategies) with:
    
    > cd java
    > export CLASSPATH="build/classes/core:build/classes/benchmarks/"
    > java -d64 -Xmx5g -cp $CLASSPATH nz.ac.massey.gp2.pointsto.benchmarks.MeasureSingleSource <path to dataset>

### Logicblox Comparison ###

Assuming you have a Logicblox license, there are a few considerations when attempting to replicate the execution times for Logicblox:

 * **Memory/Time tradeoff.** Logicblox executions in principle always terminate (though instability can be seen on low-memory machines). However, if insufficient memory is given to the system, the execution will slow down considerably. We have experimental times for systems with 128GB of memory and 32GB of memory, despite the superior processor speed of the latter, the former's Logicblox execution is faster due to the memory allowances.
 * **Analysis optimisations.** Several optimisations can be made to the datalog program `vpt.logic` used in our analysis, which would improve its performance. We have optimised the Logicblox program to the best of our abilities, however we do not perform optimisations such as on-the-fly call-graph construction, which improves runtime *and* precision, since such optimisations would similarly improve the WL, DP and TC implementations, though they have not made it.

Running Logicblox manually is a non-trivial undertaking. Users familiar with Linux internals can consult the `lb/lb.sh` script if they wish to undertake this.

### Memory and Runtime Statistics ###

Memory and runtime statistics are gathered during experimentation by external monitor programs. These programs are potentially inaccurate, but their inaccuracies are well understood in the literature.

`/usr/bin/time` is used to time experiments. Where possible we time only the points-to analysis portion of the implementation, and exclude IO times. More accurate times for the WL, DP and TC implementations is recorded internally with java's `System.nanoTime()` and logged during execution.

`memusg` is used to monitor memory footprint. Internally this script forks the process and periodically checks its process-group size using the unix internal `/bin/ps`. This mechanism is the most accurate and fair technique available to us, as it works both for our java implementations, and the proprietary Logicblox.  If more accurate memory-monitoring software is available we encourage its use for **all implementations**.

Artifact evaluation is expected to be performed on a virtual machine, which will be unable to reproduce the performance statistics seen on our bare-metal execution.

### Correctness ###

Correctness of the evaluation is defined according to the specification of java field-sensitive context-insensitive flows-to analysis, as presented by Sridharan and Bodik (2006, Refinement-based...).  All the implementations in our evaluation are 'correct' by fulfilment of this contract, and they mutually confirm the correctness of each other. Note that DP, the Difference Propogation method, does produce false negatives in certain circumstances, we are not convinced of the soudness of this analysis.

The quick (and imprecise) way to confirm correctness is to verify that the *number* of VarPointsTo relations reported by the implementation is the same. For a more accurate confirmation, the entire VarPoints To relation for some implementation/benchmark can be dumpled with the `MeasureAll` script.

    > java -cp ... MeasureAll -wl -ss -dump <benchmark> | sort -u > wl-vpt.csv
    > diff wl-vpt.csv <benchmark>/VarPointsTo.csv

Consult `MeasureAll -h` for different execution options.

All our analyses compute the same result with the *same precision*. We treat the problem of improving analysis precision as independant from our own work.

### Ideas for Extension ###

 * Implement your own analysis in the frameowrk
 * Add your own input datasets to be evaluated
 * Modify the datalog/java executions to improve precision
 * Perform C-style Andersen's analysis out-of-the-box, where the reference and dereference of variables can be seen as loads/stores to objects with a single field.

